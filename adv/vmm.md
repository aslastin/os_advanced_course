# Менеджер виртуальной памяти (VMM)

## Виртуальная память

- __Логическая концепция.__ 

- __Реализована с помощью физической памяти и VMM.__ Реализация обеспечивается физической памятью, то есть некоторый микросхемой, которая умеет достаточно быстро переключать состояния тех или иных адресуемых элементов (байтов) и адресовать по некоторому линейному индексу. ___VMM (менеджер виртуальной памяти)___ - компонента, обеспечивающая функционирование концепции виртуальной памяти для процесса, ядра.

Виртуальная память существует не только для _userland_ - места, где исполняется пользовательский код. Ядро при загрузке работает с физической памятью, но достаточно быстро включает менеджер виртуальной памяти и тоже начинает работать с виртуальной памятью, только в более высоко приоритетном контексте (режим supervisor). 

- Обеспечивает 
    - __Универсальную вычислительную модель памяти, обычно линейный массив.__ Если мы думаем о логической модели виртуальной памяти, то это разреженный массив от 0 до примерно 2^64, откуда можно получать какие-то значения. В более старых вычислительных системах существовала концепция сегментации, то есть разделение на логические подкомпоненты, в которых адресация происходила 2 частями - _сегментным регистром_, описывающим номер сегмента, и _линейным адресом из сегмента_. От этой концепции отказались, и теперь адрес - это одно число.

     Целью сегментации изначально было разделение адресного пространства на логические единицы. Например, если рассмотреть __x86__, то в нем есть такие регистры как _CS - code segment_ (любой адрес на стеке был неявно вычисляемым относительно стекового сегмента) и _SS - stack segment_. Это позволяло более адекватно структурировать программы. Поверх сегментной модели можно строить нетривиальные решения даже в современных системах, позволяющие иметь режим неявной адресации, то есть не от 0 , а некоторый условной базы. 
     
     В современных 64-битных процессорах от Intel или AMD осталось только 2 сегментных регистра - `FS`, `GS`, которые используются для сохранения базы к `ls`-данным для текущего потока, то есть, если у нас есть thread-локальная переменная, то вы можете адресовать эти данные по одному или тому же offset (пользуясь одним и тем же адресом), но используя разные сегментные регистры. Так как переключении контекстов, переключаются и значения сегментных регистров, то автоматически ваш код, работающий со thread-локальными данными, будет работать с разными данными в разных потоках. 

    - При этом нам необходимо __управлять ресурсом физической памяти__ - __выделять и освобождать__ память разными способами. Это ресурс ограниченный, поэтому нужно реализовать абстрактную удобную модель линейной памяти поверх физической памяти. 

    - Благодаря контролю над адресным пространством виртуальной памятью, можно предоставлять __больший объем физической__ памяти, чем существует на систему - обеспечить __swap технологию__ - вынесение неиспользуемых или давно не используемых блоков данных на диск и переиспользование физической страницы, которая использовалась для этого, для чего-то другого.

    - Очень важная функция виртуальной памяти -  __реализация и использование буферного кеша для файловой системы__. Из-за того, что виртуальная память позволяет делать достаточно сложные манипуляции, которые для самого процесса будут выглядеть как просто чтение или запись из того или иного адреса, возможно реализовывать файлы, отображаемые в память, под которыми лежит кеш буферов для взаимодействия с персистентным хранилищем - файловой системой. Про буферный кеш можно прочитать [здесь](https://tldp.org/LDP/sag/html/buffer-cache.html).

    - Благодаря наличию этого контроля можно оптимизировать совместное использование ресурсов. Если посмотреть на современную программу под Windows/Linux, то список кодовых зависимостей у разных программ очень часто примерно одинаковый, то есть почти все программы зависят от `lib.c`, другие от графических библиотек. Наличие виртуальной памяти позволяет создавать удобную иллюзию, что одни и те же данные доступны в разных процессах, и это можно обеспечить достаточно дешево - используя одни и те же подлежащие физические страницы, предоставлять вид, как-будто у процесса они свои. 

    В Linux есть такая уникальная страница, которая состоит из одних нулей и отображается практически во все процессы, например, если есть данные, которые инициализируются `0` операционной системой, то эта самая единственная страница замаплена в адресное пространство. Как только кто-то пытается записать туда какие-то данные в любое место страницы, случается __fault__ (страница защищена за запись), и аллоцируется новая страница, куда реально пишутся данные - __COW (Copy On Write)__.


## Адресное пространство

![2.1]()

Тот самый разреженный массив слева - виртуальное адресное пространство, в котором сегментация на текст/данные/стек осталась логическая - есть некоторые переменные куски адресного пространства, которые посвящены использованию тех или иных фрагментов программы. 

Если посмотреть на физическую память, то отображение адресного пространства достаточно сложно, то есть то, какая конкретная страница используется для данного логического адреса, известно только ядру и потенциально может меняться. 

Более того, иногда такой страницы вообще может не быть, если произошел _своппинг - вынесение страницы, принадлежащей процессу на диск_. Тогда данная страница предоставлена какому-то другому процессу и в этот момент стрелка на рисунке указывает в никуда, точнее на страницу, любой доступ к который ведет к __fault__ и к обращению в структуру ОС, которая говорит, где на диске хранятся данные соответствующие данному адресу. 


## Адресное пространство, трансляция

![2.2]()

Рассмотрим процесс трансляции виртуальных адресов. Логически говоря, у нас есть виртуальный адрес, который является индексом в этом самом условном адресном пространстве. Этот адрес делится на 2 части: некоторое количество нижних битов - _сдвиг страницы_, то есть, если мы что-то знаем про физический адрес то это то, что последние 12 битов физического адреса совпадают с последними 12 битами логического адреса. А вот верхняя часть адреса используется для осуществления трансляции адреса, которая производится способом различным для каждого адресного пространства. 

Поэтому, если мы хотим оттранслировать тот или иной виртуальный адрес, мы берем верхнюю часть этого адреса, консультируемся с таблицей страниц и на основании информации из этой страницы вычисляем, куда попадает данный адрес в физическую память, но возможны и следующе исходы:

- адрес может никуда не попадать.
- исключение ОС, если не имеем права обращаться к этому адресу.
- трансляция не успешна, потому что ОС отправила своп нашей страницы. Однако ОС знает об этом факте, поэтому она поднимает страницу из свопа, мапит куда-то, и трансляция после этого наконец получается. 


## Таблица трансляции

![3.1]()

Здесь указана таблица трансляции для 64-битного адресного пространства, потому что она более интересна и показывает концепции, которые нельзя увидеть на 32-битной системе. 

Таблицу трансляции строит ОС, а использует обычно [MMU](https://habr.com/ru/post/211150/).
 
Первый уровень - __PGD__ (терминология Linux). Структурно это дерево поиска, где каждая часть адреса отвечает за индексацию в своей компоненте. 

Входом для данной трансляции является базовый адрес таблицы страниц для текущего процесса и сдвиг в этой таблице. Каждый элемент этой страницы - одна физическая страница, то есть на индексацию выделяется физическая страница.

Таким же образом мы берем следующий кусок адреса (30 - 38) и индексируемся в __PUDE__. 

Далее берем следующий кусочек адреса - __PMD__. 

И наконец дошли до __PT__. __PT__ - то место, которое говорит, где же на самом деле находится страница. Кроме этого, там хранится дополнительная информация о том, какие права доступа к этому адресу в рамках данной системы понятий (одна и та же физическая страница может быть по-разному видна процессору, в зависимости от того, что сейчас записано в регистре CR3), то есть могут быть такие правила трансляции при которых одному процессу страничка кажется `read-only` и при попытке записи в нее происходит _fault_, а другому процессу она `read-write`. Эта вариативность обеспечивается такой гибкой системой трансляции.

Биты с 63 - 48 никак не используются, ведь реальный размер виртуального адресного пространства меньше, чем 64 бита, из-за того, что, для того чтобы реализовать реальный lookup такой глубины (здесь мы реализовали 12 битов на сдвиг и 36 бита на остальное и по итогу получилось 48 бита). Реальное адресуемое виртуальное адресное пространство чаще всего имеет ширину 46 бита (можно проверить, пытаясь менять значение в верхних частях адреса и смотреть, будут ли ломаться указатели или нет). 


## Устройство Page Directory Entry (PDE) x86

![3.2]()

Что собственно записано в __Page Directory__? 

Часть записи отвечает за адрес физической страницы. Из-за того, что физический адрес начала страницы всегда выровнен (всегда нижние 12 бит нули), поэтому оставшиеся биты отвечают за то, каким образом можно общаться с тем, что мы данной страничкой адресуем.

__S__, или "размер страницы", хранит размер страницы для этой конкретной записи. Если бит установлен, то страницы имеют размер 4 MiB. В противном случае они составляют 4 KiB.

__A__, или "доступ" используется для определения того, была ли страница прочитана или записана. Если была, то бит установлен, в противном случае его нет. Этот бит не будет очищается процессором, так что это задача ложится на ОС (если она вообще нуждается в этом бите). Вдобавок, наличие этого бита позволяет реализовывать алгоритмы вытеснения, наименее используемых страниц. 

__D__, это бит "запрет кеширования", то есть любой доступ к этой странице должен происходить на прямую, а не храниться в кешах процессора. Это важно для ситуаций, когда данный виртуальный адрес отображает регистры того или иного устройства, например, видеокарт или сетевой карты. Тогда запись или чтение из того или иного адреса - та или иная команда оборудования, поэтому ее нельзя кешировать. Если эту команду послали - она должна случиться, оттого необходимо запрещать кеширование в этот момент. Если бит установлен, страница не будет кэшироваться, иначе будет.

__W__, управляет "write-through" возможностями страницы. Write through - политика записи, позволяющая откладывать запись. Если бит установлен, то включено write-through кэширование записи. Если нет, то вместо этого включена обратная запись.

__U__, бит "пользователь/Супервизор", управляет доступом к странице на основе уровня привилегий. Если бит установлен, то страница может быть доступна всем; если бит не установлен, только супервизор может получить доступ к ней. Для записи каталога страниц бит пользователя управляет доступом ко всем страницам, на которые ссылается запись каталога страниц. Поэтому, чтобы сделать страницу пользовательской, надо установить бит пользователя в соответствующей записи каталога страниц, а также в записи таблицы страниц.

__R__, флаг разрешений "чтение/запись". Если бит установлен, то страница читается/записывается. В противном случае, страница доступна только для чтения. Бит __WP__ в __CR0__ определяет, применяется ли это только к userland, всегда предоставляя ядру доступ на запись (по умолчанию) или как к userland, так и к ядру.

__Р__, или "Present". Если бит установлен, то страница фактически находится в физической памяти в данный момент. Например, когда страница заменяется, она не находится в физической памяти и, следовательно, не "присутствует". Если страница вызывается, но отсутствует, произойдет ошибка страницы, и ОС должна ее обработать.

Остальные биты с 9 по 11 не используются процессором и являются бесплатными для ОС для хранения некоторой собственной учетной информации. Кроме того, когда __P__ не задано, процессор игнорирует остальную часть записи, и можно использовать все оставшиеся 31 бит для дополнительной информации, например, записи того, где страница оказалась в swap space.


## Устройство Page Table Entry (PTE) x86

![3.3]()

Первый часть записи - это опять же физический адрес, выровненный по 4 KiB. Однако, в отличие от предыдущих версий, адрес не является адресом таблицы страниц, а представляет собой блок физической памяти размером 4 KiB, который затем сопоставляется с этим местоположением в таблице страниц и каталоге.

__C__, или кэшированный, - Это бит __D__ из предыдущей таблицы.

__G__, или глобальный флаг, если он установлен, запрещает TLB обновлять адрес в своем кэше, если CR3 сбрасывается. Говорит, что данная страница присутствует и в одном и в другом адресном пространстве по одному и тому же адресу при переключении контекста, поэтому можно не инвалидировать в TLB и кешах, относящихся к данной странице (исключительно для оптимизации).

__D__, или грязный флаг, если установлен, указывает на то, что страница была записана. Этот флаг не обновляется процессором, и после установки он не будет сброшен сам по себе.

Типичная ситуация в процессорах, что некоторые биты остаются под будущее расширение, то есть он в данный момент является 0 (AMD - MBZ - Must Be Zero), а в процессорах  типичное поведение такое - есть некоторое старое default поведение, при добавлении в процессор новой фичи можно активировать новое поведение, а старые программы обязаны писать туда 0, поскольку о новом поведении не знают.


## Трансляция адресов на ARM

![3.4]()

Начнем с 32 битной версии.

Аналогом регистра __CR3__ является __TTBR0__ или __TTBR1__, в зависимости от того, где производится трансляция, в каком режиме процессора (ядерном/пользовательском). Это также регистры, указывающие на корень таблицы трансляции, при этом эта самая запись может указывать на несколько разных сущностей, то есть это дерево менее однородное в сравнении с __x86__: секцию - 1 мегабайтный регион памяти, таблицу страниц следующего уровня, супер секцию, указывающую на 16 мегабайтный регион памяти. Таблица страниц может из себя представлять указатель либо на большую страницу (64 KiB), либо на маленькую (4 KiB).


## PTE для arm v7

![3.5]()

Индивидуальная запись в таблице страниц на arm v7 может быть устроена, как показано на картинке: дифференциация происходит по младшим битам: 

- <0, 0> - некорректная запись -> случится fault; 

- <0, 1> - указатель на page table, далее есть описание того, где находится page table и некоторые дополнительные биты; 

- указатель на секцию, есть дополнительная информация специфичная для данной секции;

- указатель на супер секцию, тогда базовый адрес занимает еще меньше места, тем самым остается больше места для дополнительной информации. 


## Трансляция адресов на ARM

![3.6]()

Схема еще более похожа на __x86__: 3-х уровневый lookup, адрес разбит на кусочки, только используются page descriptor, table descriptor и еще одна такая пара, и различные куски виртуального адреса имеют разную семантику:  12 бит - сдвиг внутри физической страницы, остальное - индексы в соответствующих логических страницах.


## PTE для arm v8

![3.7]()

Индивидуальная запись для arm v8 имеют похожую структуру: глядя на нижний бит можно проверить валидность записи, точно также есть информация о типе странице и присутствует дополнительная информация о трансляции данного адреса.


## TLB - Table Lookaside Buffer

![3.8]

Важная и полезная концепция в функционировании VMM - ___TLB___ - кеш трансляции виртуальных адресов в физические.

Если мы имеем некоторый процесс, имеющий некоторые виртуальные адреса, то благодаря предыдущей схеме, можно оттранслировать их в некий физический адрес. Однако если любой доступ к памяти будет занимать 3 доступа к памяти на чтение каждого уровня, индексацию, проверки, то процессор будет работать очень медленно, поэтому, чтобы сделать трансляцию максимально дешевой, в MMU реализована на уровне кремния концепция как TLB, который позволяет запоминать, что данный виртуальный адрес соответствует данному физическому.

На картинке изображена дополнительная оптимизация. Планировщик осуществляет переключение процессов, в частности он должен делать переключение правил трансляции - того самого базового регистра для трансляции и поэтому, если просто наивно переключать процессы, одной и той же странице может соответствовать одна физическая страница, а тому же самому виртуальному адресу в другом процессе может соответствовать абсолютно другая страница, отсюда и надо сбрасывать. 

Дабы избежать сбрасывание из-за того, что поиск в TLB гораздо дешевле (кеш на процессоре), чем реальное хождение в физическую память для выяснения правил трансляции, иногда сохраняют _идентификатор адресного пространства_ или _идентификатор процесса (address face id)_, который является вторым ключом в TLB. Благодаря этому, глядя на то, какой сейчас текущий процесс на процессоре и какая виртуальная страница, можно выяснять номер физической страницы.

Важное свойство TLB, что это критическая компонента для производительности процессора, поэтому ему необходимо иметь очень высокий hit rate. ___Hit Rate___ - характеристика любого кеша -  то с какой частотой в типичном сценарии исполнения программы запросы будут удовлетворяться из кеша, а не идти мимо по более длинному пути. Для TLB данная характеристика должна быть порядка 99% и даже выше - важно иметь качественную реализацию TLB.


## VMM и IO

![3.9]()

Картинка выше полезна для понимания, каким образом в функционировании ядра ОС существует VMM. Нам более интересна верхняя часть этой картинки - нижняя это описание ввода-вывода.

Верхняя часть - процесс. У процесса есть интерфейсы системного уровня, такие как прочитать что-то из файлового дескриптора(fd), записать что-то в fd, создать fd с помощью `open` и такие операции как `mmap` - выделения кусочка памяти для дальнейшего использования этим процессом.

Кеш страниц - та часть VMM, которая участвует в вводе-выводе, то есть собственно, если мы запускаем процесс, он как-то задействуется. 

Например, есть написанный исполняемый файл, допустим `bin/ls`. Мы пишем в консоли `ls` - происходит - командный shell - командный интерпретатор находит, где команда `ls` находится на диске и говорит ядру при помощи последовательности `fork/exec` или `pspawn` - давайте выполним эту команду в дочернем от меня процессе. 

Как это собственно происходит? Интересная часть - команда `exec`, которая заменяет текущее адресное пространство отображением из данного исполняемого файла. В исполняемом файле присутствуют различные сегменты, такие как сегменты кода, сегменты данных, и собственно эти сегменты соответствуют данным на диске. Допустим мы начали исполнение, тогда мы берем кусочек файла, который соответствует исполняемой команде, а именно тот кусок, который описан как entry point - точка входа -  адрес точки входа для данного процесса, и отображаем ее в память - говорим, что теперь по некоторому адресу, который записан в исполняемом формате, должны находиться те самые данные, которые в этом файле по этому offset. 

И дальше, мы отображаем весь файл куда-то в память, но при этом мы ничего оттуда не читаем, а просто передаем туда исполнение, и первым делом встречается fault, как только мы передали исполнение на процесс. В обработчике fault ОС видит, что здесь должен быть файл `bin/ls` замапленный с определенного offset, значит я подгружу соответствующую страницу, после чего продолжу исполнение. Подгружу - пройдем через manager блочного ввода-вывода, который соответственно обратится к соответствующей реализации файловой системы, затем обратимся к тому или иному адресному хранилищу - будь это диск не так важно, все равно логическая модель для взаимодействия с драйверами одна и та же - "Дай мне данный блок!" Поэтому мы получаем этот блок и записываем его в память и вот эта страница в памяти оказывается видной для процесса как что-то чем уже можно пользоваться, соответственно происходит дальше исполнение, и если оно достигает снова странички неподмапленной в память, то происходит этот же процесс и дальнейшее исполнение. 

Как можно увидеть, VMM объединяет функционирование кода приложения с остальными частями ОС и обеспечивает кеширование - если мы знаем, что страничка подмаплена read-only и уже кем-то этот файл, например, `bin/ls` сейчас используется, либо был недавно использован, то мы можем сказать: "Вот тебе страничка, я ничего читать не буду, потому что это все у меня уже закешировано."


## Организация VMM

Что VMM представляет из себя как алгоритм и программная компонента? 

- __Содержит:__

    - __Таблицу используемой физической памяти.__ Мы используем массив физической памяти сложным образом, не так как об этом думают процессы. Дело VMM выступать здесь как allocator страниц.

    - __Список регионов виртуальной памяти каждого процесса (VMA).__ У процессов есть сегмента кода/данных, другие куски памяти, подмапленные с помощью `mmap`, и такой непрерывный фрагмент из адресов называется __VMA (Virtual Memory Area)__.

    - __LRU (Last Recently Used) кеш использованных страниц__ - кеш страниц, которые часто/недавно использовались, а те, которые уже не нужны из кеша выкидываются и замещаются для каких-то других целей. Это компонента, напрямую взаимодействующая с менеджером физической памяти, потому что именно в физической памяти происходит выделение этих страниц. Чтобы ядру не нужно было, оно берет это из физической памяти, а дело менеджера виртуальной памяти предоставлять эти физические страницы по запросу.

    - __Отображение памяти устройств MMIO, регионы DMA__. VMM должна поддерживать трансляцию физической памяти, которая является тем, куда можно писать/записывать данные произвольным образом без каких-либо дальнейших последствий. Есть такие устройства как сетевые карты, графические ускорители, у которых тоже есть выданные им регионы физической памяти, про которые VMM должен знать и никогда не выделять как реальные страницы и при этом выделять их, если нужно отобразить в ядро или в тот или иной процесс. Если говорить про gpu, отображение памяти gpu в процесс достаточно общее место, то есть очень многие программы, особенно те, которые работают с графикой, отображают в свое адресное пространство кусочек памяти gpu, чтобы туда копировать текстуры, читать что-то отрендированное. 
    
    Управление регионами __DMA (Direct Memory Access)__ - концепция, связанная с реализацией ввода-вывода. 
    
    Если что-то нужно получить от периферийного устройства, то есть несколько политик это сделать, например, можно попросить устройство это сделать, долго ждать пока устройство это делает, ничего не делать больше, а потому уже продолжить исполнение со сделанными файлами (такая модель реализованна логически в системном вызове `read` - синхронное чтение). Во взаимодействием с реальными устройствами на уровне ядра, это очень неэффективная политика, потому что очень долгая. Периферия значительно медленнее, чем скорость исполнения самого процессора, поэтому реализована такая концепция как __DMA__ - механизм, позволяющий создавать запросы к устройствам и давать им память для исполнения этого запроса. 
    
    Например, дисковое устройство - прочитай мне 25 блок и запиши его по физическому адресу 123. И именно после того, как случится запись по адресу 123 со стороны устройства, устройство поднимет прерывание и процессор сможет продолжить исполнение тех задач, которые заблокированы на чтение из файла, и при этом в это же время процессор может делать что-то полезное - практически все сложные задачи будут сделаны самим устройством, что значительно повышает производительность.

    - __Аллокатор памяти ядра (kmalloc).__ Самому ядру тоже нужно с чем-то работать, поэтому в операционном ядре есть такая функция как `kmalloc`. В ядре ОС есть 2 функции выделения памяти: `bmalloc` - выделяет страничку или набор страниц; `kmalloc` - выделяет с меньшей гранулярностью, то есть если нужно создать связный список, то элементы такого связного списка будут создаваться с помощью `kmalloc`. 

    - __Кеш блоков для ввода/вывода.__ Для того чтобы осмысленно взаимодействовать с внешними устройствами нужно уметь возвращенные данные кешировать и предоставлять по запросу в виде регионов физической памяти.

    - __Менеджер файлов отображаемых в память (mmap(2)).__ Некоторые регионы внутри адресного пространства процесса отображены на файлы. Есть такой очень сложный системный вызов (самый сложный в UNIX) как `mmap` (6 аргументов), который предоставляет интерфейс примерно половине того, что делает ядро ОС UNIX. Отображение файлов в память одна из ключевых ответственностей и способностей UNIX подобных ОС, позволяющая делать примерно все остальное. 

Для более глубокого понимания читать [тут](https://www.kernel.org/doc/gorman/pdf/understand.pdf).


## Управление страницами, vma, rmap

![3.10]()

Как же мы управляем страницами в VMM? 

Устройство физической памяти, то есть то, чем мы управляем это на левой части фото - набор кусочков RAM, затем есть регионы, отвечающие за memory mapped I/O - ввод/вывод отображаемый в память - концепция, позволяющая представить операции ввода/вывода таким образом, как-будто это были операции чтения/записи из памяти. Практически весь ввод/вывод в современных ОС производится при помощи memory mapped I/O и прерывании как парной концепции. 

Достаточно часто видеокарты интерпретируются специальным образом из-за того, что на них хранятся гигантские объемы данных, то есть если для типичной сетевой карты пространство memory mapped I/O это несколько килобайт, то видеокарта может предоставлять много мегабайтные регионы для взаимодействия с теми данными, которыми нужно загружать на видеокарту, поэтому обычно в __x86__ есть специальная трактовка памяти связанной с видеокартой, но логически это тоже memory mapped I/O с некоторыми дополнениями. 

Логика в микросхеме умеет дифференцировать является ли тот иной доступ доступом к настоящей физической памяти, либо он отвечает за взаимодействие с устройством - это сугубо аппаратный механизм, здесь менеджер виртуальной памяти выступает исключительно как клиент для аппаратной службы, а соответственно для него квантами управления являются VMA, то есть диапазон виртуальной памяти для того или иного процесса. Это собственно и позволяет управлять памятью в рамках всего ядра.

Одна из сложных задач, которую реализует ОС - возможность обратной трансляции. Как мы видели, ядро поддерживает отображение, где ключом является виртуальный адрес, а значением является физический адрес. Но достаточно не редка ситуация, когда необходимо решить обратную задачу: какому виртуальному региону принадлежит данная физическая страница - rmap - отображение, обратное к трансляции страниц. Это отображение очень сложно поддерживать в ядре. 

В Linux это реализовано через механизм, позволяющий найти по структуре страницы через указатель на анонимное VMA. Если VMA то, что присуще конкретному процессу, то можно агрегировать кусочки VMA, которые отвечают за одни и те же физические страницы и хранить указатель на анонимизированный VMA в структуре, описывающей физическую страницу. В результате можно эффективно поддерживать обратное отображение. 


## Кеш страницы, LRU

![3.11]()

Что мы пытаемся достичь, кешируя страницы? Скомпенсировать низкую скорость периферийных устройств. Обычно, второй или третий запуск программы быстрее первого. Причина этой быстроты - наличие кеша страниц. 

Технически это значит, что взаимодействие с диском достаточно сложный процесс, проходящий через менеджер файловой системы, через уровень блочного ввода/вывод, через конкретные драйвера для контроллеров и наконец через аппаратное обеспечение, также не быстрое и должно считывать запрошенные данные соответствующим образом. Для компенсации сложности схемы, необходимо уметь те страницы, которые мы можем предоставить быстро - предоставлять быстро. Для этого используется кеш страниц, в котором используется алгоритмическое свойство как __LRU__ - пытаемся поддерживать кеш страниц. Вначале в кеше только 1 страница, дальше поддерживаем момент последнего использования. 

На картинке у нас есть A, B, C, D, E, F и с каждым из них атрибутирован момент последнего использования. Когда случается ситуация, что мы не можем больше выделить страниц под кеш страниц без удаления какой-либо существующей, происходит удаление последней использованной страницы.

Для кеша страниц есть 3 типа данных, с которыми он работает: _файловые страницы_, _анонимные страницы_ и _неосвобождаемые страницы_.

_Файловые страницы_ - под ними есть некий файл. Если элемент нужно выкинуть из кеша, то мы синхронизируем состояние данной страницы с подлежащим файлом, после чего говорим, что страница свободна; 

_Анонимные страницы_ - та память, под которой нет файла, то есть посвящена стеку, хипу. Такую память, если просто выкинуть, то непонятно, как ее восстановить, поэтому для таких страниц создается виртуальный маппинг в swap и в зависимости от содержания освобождается swap. _Swap_ - кеш страниц для анонимных страниц. При помощи вызова `mlock` и `munlock` можно сделать страницу, не попадающей в swap. Содержательно, если мы пишем какой-то криптографический алгоритм и у нас есть частные ключи, то люди обычно не хотят, чтобы они казались в swap, то есть на персистентом носителе, так как существуют атаки на машины, когда ценные и содержательные данные выдираются из swap данных;   

_Неосвобождаемые страницы_ - страницы, которые нельзя выгружать на диск. 


## Память ядра, kmalloc

![3.12]()

Выделение памяти для нужд ядра является специальной операцией.  

Для пользовательских процессов, выделение памяти - один из сервисов ядра, а для ядра логично, что ядро не может обеспечить такой сервис, поэтому ему приходится идти на ухищрения и поддерживать внутри себя _аллокатор_ - что-то, умеющее выделять и освобождать память небольших размеров. Память с гранулярностью страницы выделяется с помощью`vmalloc`, а маленькая память выделяется с помощью `kmalloc`. `kmalloc` реализован чаще всего при помощи алгоритма хорошо показавшего себя - [buddy allocator](https://en.wikipedia.org/wiki/Buddy_memory_allocation). 

Если мы говорим про пользовательские процессы, то про них фрагментация неприятный момент, но обычно достаточно некритичный по той причине, что процесс рано или поздно заканчивается, то есть типичный процесс имеет ограниченное время жизни (есть долгоживущие процессы - applicatoin servers). 

_Фрагментация памяти_ - выделение маленьких кусочков и невозможность выделения ничего больше из-за того, что освобожденные кусочки находятся между выделенными. 

Для ядра эта ситуация неприемлема, потому что это значит, что после наступления фрагментации, единственное, что можно сделать это перезагрузить систему, поэтому поддерживается аллокатор, суть которого в том, чтобы минимизировать фрагментацию. 

Это достигается методом _buddy allocator_ - все выделения группируются по размерам - если мы просим 15 байтов - выделяется 16 байт и размеры сложены по степеням двойки. 7 байт - 8 байт выделяется. Дальше _арена_ - место подходящее под аллокацию - делится на степенные арены и их можно дефрагментировать при необходимости, так как всегда 2 арены предыдущей степени двойки создают из себя полноценную арену следующей степени двойки. 



